{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset pairs created and saved as car_pairs.csv!\n"
     ]
    }
   ],
   "source": [
    "# Path to the image dataset directory\n",
    "root_dir = \"/data/NNDL/data/image\"\n",
    "\n",
    "# Create a dictionary to group images by car make\n",
    "car_make_dict = {}\n",
    "\n",
    "for dirpath, _, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            # Extract car make from the path (3rd-level directory)\n",
    "            parts = dirpath.split(os.sep)\n",
    "            car_make = parts[-3]  # The 3rd-level directory is the car make\n",
    "            \n",
    "            if car_make not in car_make_dict:\n",
    "                car_make_dict[car_make] = []\n",
    "            car_make_dict[car_make].append(os.path.join(dirpath, file))\n",
    "\n",
    "# Function to create positive and negative pairs\n",
    "def create_pairs(car_make_dict, num_pairs=10000):\n",
    "    pairs = []\n",
    "    makes = list(car_make_dict.keys())\n",
    "    \n",
    "    for _ in range(num_pairs):\n",
    "        # Positive pair (same car make)\n",
    "        make = random.choice(makes)\n",
    "        if len(car_make_dict[make]) >= 2:\n",
    "            img1, img2 = random.sample(car_make_dict[make], 2)\n",
    "            pairs.append((img1, img2, 0))  # label 0 for similar\n",
    "        \n",
    "        # Negative pair (different car makes)\n",
    "        make1, make2 = random.sample(makes, 2)\n",
    "        img1 = random.choice(car_make_dict[make1])\n",
    "        img2 = random.choice(car_make_dict[make2])\n",
    "        pairs.append((img1, img2, 1))  # label 1 for dissimilar\n",
    "\n",
    "    return pairs\n",
    "\n",
    "# Generate 10,000 pairs (adjust num_pairs as needed)\n",
    "pairs = create_pairs(car_make_dict, num_pairs=10000)\n",
    "\n",
    "# Convert to a DataFrame for easier processing\n",
    "pairs_df = pd.DataFrame(pairs, columns=[\"img1\", \"img2\", \"label\"])\n",
    "\n",
    "# Save the pairs to a CSV file\n",
    "pairs_df.to_csv(\"car_pairs.csv\", index=False)\n",
    "print(\"Dataset pairs created and saved as car_pairs.csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data augmentation and normalization for training and validation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.RandomRotation(15),  # Random rotation within 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random changes in brightness, contrast, etc.\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop and resize\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
    "    transforms.RandomGrayscale(p=0.1),  # Randomly convert to grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize to ImageNet stats\n",
    "])\n",
    "\n",
    "# Custom dataset class for loading and preprocessing image pairs\n",
    "class CarPairDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.pairs_df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.pairs_df.iloc[idx]\n",
    "        img1_path = row[\"img1\"]\n",
    "        img2_path = row[\"img2\"]\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "\n",
    "        img1 = self.load_image(img1_path)\n",
    "        img2 = self.load_image(img2_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return (img1, img2), label\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"Load an image from a given path and convert it to PIL format.\"\"\"\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not read {image_path}\")\n",
    "            img = np.zeros((299, 299, 3), dtype=np.uint8)  # Return a black image if loading fails\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 16000, Validation set size: 4000\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "csv_file = \"car_pairs.csv\"  # Update this to your actual CSV file path\n",
    "car_dataset = CarPairDataset(csv_file=csv_file, transform=transform)\n",
    "\n",
    "\n",
    "# Total dataset size\n",
    "total_size = len(car_dataset)\n",
    "\n",
    "# Split sizes: 80% for training, 20% for validation\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# Randomly split the dataset\n",
    "train_dataset, val_dataset = random_split(car_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True , num_workers= 16, pin_memory=True, prefetch_factor=2, persistent_workers=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False  , num_workers= 16, pin_memory=True, prefetch_factor=2, persistent_workers=True)\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset)}, Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Use ResNet50 pretrained on ImageNet as the feature extractor\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Load pretrained ResNet50 and remove the last fully connected layer\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet50.children())[:-1])  # Remove the FC layer\n",
    "\n",
    "        # Add a fully connected layer for feature comparison (optional)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        \"\"\"Pass the input through the feature extractor.\"\"\"\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        \"\"\"Compute embeddings for both inputs and return the distance.\"\"\"\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision.models as models\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # Load pretrained Inception v3 model\n",
    "#         inception = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "        \n",
    "#         # Remove the last fully connected layer\n",
    "#         self.feature_extractor = nn.Sequential(\n",
    "#             *list(inception.children())[:-1],  # Remove FC layer\n",
    "#             nn.AdaptiveAvgPool2d((1, 1))  # Ensure consistent output shape\n",
    "#         )\n",
    "        \n",
    "#         # Fully connected layers for embedding\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(2048, 512),  # Inception's last layer outputs 2048 features\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 128)\n",
    "#         )\n",
    "\n",
    "#     def forward_once(self, x):\n",
    "#         \"\"\"Pass the input through the feature extractor.\"\"\"\n",
    "#         x = self.feature_extractor(x)  # Extract features\n",
    "#         x = torch.flatten(x, 1)  # Flatten to (batch_size, 2048)\n",
    "#         #x = self.fc(x)  # Pass through FC layers\n",
    "#         return x\n",
    "    \n",
    "#     def forward(self, input1, input2):\n",
    "#         \"\"\"Compute embeddings for both inputs and return the distance.\"\"\"\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         return output1, output2\n",
    "\n",
    "\n",
    "\n",
    "# Define the contrastive loss function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        \"\"\"Compute contrastive loss.\"\"\"\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "# # Define the Triplet Loss function\n",
    "# class ContrastiveLoss(nn.Module):\n",
    "#     def __init__(self, margin=1.0):\n",
    "#         super(ContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "\n",
    "#     def forward(self, anchor, positive, negative):\n",
    "#         \"\"\"Compute Triplet Loss\"\"\"\n",
    "#         pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
    "#         neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
    "        \n",
    "#         loss = torch.mean(torch.clamp(pos_dist - neg_dist + self.margin, min=0.0))\n",
    "#         return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/data/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the model, loss function, and optimizer\n",
    "model = SiameseNetwork().cuda()  # Move model to GPU if available\n",
    "criterion = ContrastiveLoss(margin=1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)  # Reduce LR if no improvement\n",
    "\n",
    "# Training loop with validation using the split dataset\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "patience = 5  # Early stopping patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 125/125 [01:39<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 0.2182, Training Accuracy: 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1920, Validation Accuracy: 0.7020\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 125/125 [01:31<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Training Loss: 0.1777, Training Accuracy: 0.7330\n",
      "Validation Loss: 0.1769, Validation Accuracy: 0.7365\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 125/125 [01:34<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Training Loss: 0.1610, Training Accuracy: 0.7661\n",
      "Validation Loss: 0.1689, Validation Accuracy: 0.7485\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 125/125 [01:32<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Training Loss: 0.1491, Training Accuracy: 0.7916\n",
      "Validation Loss: 0.1629, Validation Accuracy: 0.7680\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 125/125 [01:32<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Training Loss: 0.1384, Training Accuracy: 0.8093\n",
      "Validation Loss: 0.1626, Validation Accuracy: 0.7662\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 125/125 [01:32<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Training Loss: 0.1270, Training Accuracy: 0.8317\n",
      "Validation Loss: 0.1592, Validation Accuracy: 0.7712\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 125/125 [01:31<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Training Loss: 0.1200, Training Accuracy: 0.8400\n",
      "Validation Loss: 0.1598, Validation Accuracy: 0.7670\n",
      "Early stopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 125/125 [01:32<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Training Loss: 0.1119, Training Accuracy: 0.8593\n",
      "Validation Loss: 0.1591, Validation Accuracy: 0.7740\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 125/125 [01:32<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Training Loss: 0.1037, Training Accuracy: 0.8741\n",
      "Validation Loss: 0.1583, Validation Accuracy: 0.7660\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 125/125 [01:31<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Training Loss: 0.0963, Training Accuracy: 0.8848\n",
      "Validation Loss: 0.1559, Validation Accuracy: 0.7778\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 125/125 [01:31<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Training Loss: 0.0904, Training Accuracy: 0.8949\n",
      "Validation Loss: 0.1585, Validation Accuracy: 0.7688\n",
      "Early stopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 125/125 [01:32<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Training Loss: 0.0854, Training Accuracy: 0.9019\n",
      "Validation Loss: 0.1567, Validation Accuracy: 0.7722\n",
      "Early stopping counter: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 125/125 [01:32<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Training Loss: 0.0807, Training Accuracy: 0.9124\n",
      "Validation Loss: 0.1494, Validation Accuracy: 0.7877\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 125/125 [01:32<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Training Loss: 0.0738, Training Accuracy: 0.9241\n",
      "Validation Loss: 0.1510, Validation Accuracy: 0.7795\n",
      "Early stopping counter: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 125/125 [01:31<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Training Loss: 0.0706, Training Accuracy: 0.9272\n",
      "Validation Loss: 0.1551, Validation Accuracy: 0.7775\n",
      "Early stopping counter: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 125/125 [01:32<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Training Loss: 0.0653, Training Accuracy: 0.9361\n",
      "Validation Loss: 0.1574, Validation Accuracy: 0.7732\n",
      "Early stopping counter: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 125/125 [01:32<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Training Loss: 0.0611, Training Accuracy: 0.9466\n",
      "Validation Loss: 0.1609, Validation Accuracy: 0.7662\n",
      "Early stopping counter: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 125/125 [01:31<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Training Loss: 0.0494, Training Accuracy: 0.9627\n",
      "Validation Loss: 0.1530, Validation Accuracy: 0.7815\n",
      "Early stopping counter: 5/5\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Training phase\n",
    "    for (img1, img2), labels in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        img1, img2, labels = img1.cuda(), img2.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img1, img2)\n",
    "        \n",
    "        loss = criterion(output1, output2, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        similarity_scores = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        predictions = (similarity_scores > 0.5).float()  # Assuming 0.5 as the threshold\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions / total_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val_predictions = 0\n",
    "    total_val_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (val_img1, val_img2), val_labels in val_dataloader:\n",
    "            val_img1, val_img2, val_labels = val_img1.cuda(), val_img2.cuda(), val_labels.cuda()\n",
    "            val_output1, val_output2 = model(val_img1, val_img2)\n",
    "            val_loss += criterion(val_output1, val_output2, val_labels).item()\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            val_similarity_scores = torch.nn.functional.pairwise_distance(val_output1, val_output2)\n",
    "            val_predictions = (val_similarity_scores > 0.5).float()\n",
    "            correct_val_predictions += (val_predictions == val_labels).sum().item()\n",
    "            total_val_samples += val_labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_val_predictions / total_val_samples\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Early stopping and model saving\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), \"make_verification_resnet50.pth\")\n",
    "        print(\"Best model saved!\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"Early stopping counter: {early_stop_counter}/{patience}\")\n",
    "\n",
    "    # Reduce learning rate if validation loss doesn't improve\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    if early_stop_counter >= patience:\n",
    "        print(\"Early stopping triggered. Training stopped.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the test dataset class\n",
    "# class TestCarPairDataset(Dataset):\n",
    "#     def __init__(self, txt_file, root_dir, transform=None):\n",
    "#         self.pairs = []\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         # Read the text file and parse image pairs\n",
    "#         with open(txt_file, \"r\") as file:\n",
    "#             lines = file.readlines()\n",
    "            \n",
    "#         for line in lines:\n",
    "#             img1_rel, img2_rel, label = line.strip().split()\n",
    "#             img1_path = os.path.join(root_dir, img1_rel)\n",
    "#             img2_path = os.path.join(root_dir, img2_rel)\n",
    "#             label = 1 - int(label)  # Swap labels (1 -> 0, 0 -> 1)\n",
    "#             self.pairs.append((img1_path, img2_path, label))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.pairs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img1_path, img2_path, label = self.pairs[idx]\n",
    "#         label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "#         img1 = self.load_image(img1_path)\n",
    "#         img2 = self.load_image(img2_path)\n",
    "        \n",
    "#         if self.transform:\n",
    "#             img1 = self.transform(img1)\n",
    "#             img2 = self.transform(img2)\n",
    "        \n",
    "#         return (img1, img2), label\n",
    "\n",
    "#     def load_image(self, image_path):\n",
    "#         \"\"\"Load an image from the given path and convert it to PIL format.\"\"\"\n",
    "#         img = cv2.imread(image_path)\n",
    "#         if img is None:\n",
    "#             print(f\"Warning: Could not read {image_path}\")\n",
    "#             img = np.zeros((224, 224, 3), dtype=np.uint8)  # Return a black image if loading fails\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "#         return Image.fromarray(img)\n",
    "\n",
    "# # Define the same transformation used during training\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test dataset class\n",
    "class TestCarPairDataset(Dataset):\n",
    "    def __init__(self, txt_file, root_dir, transform=None):\n",
    "        self.pairs = []\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Read the text file and parse image pairs\n",
    "        with open(txt_file, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            img1_rel, img2_rel, _ = line.strip().split()\n",
    "            img1_path = os.path.join(root_dir, img1_rel)\n",
    "            img2_path = os.path.join(root_dir, img2_rel)\n",
    "            \n",
    "            # Extract the first directory (number) from each image path\n",
    "            id1 = img1_rel.split(\"/\")[0]\n",
    "            id2 = img2_rel.split(\"/\")[0]\n",
    "            # If the first directory is the same, label is 0; otherwise, label is 1\n",
    "            label = 0 if id1 == id2 else 1\n",
    "            \n",
    "            self.pairs.append((img1_path, img2_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path, label = self.pairs[idx]\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        img1 = self.load_image(img1_path)\n",
    "        img2 = self.load_image(img2_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        \n",
    "        return (img1, img2), label\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"Load an image from the given path and convert it to PIL format.\"\"\"\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not read {image_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)  # Return a black image if loading fails\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        return Image.fromarray(img)\n",
    "\n",
    "# Define the same transformation used during training\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_837388/240913420.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"make_verification_resnet50.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = SiameseNetwork().cuda()\n",
    "model.load_state_dict(torch.load(\"make_verification_resnet50.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the test dataset text file\n",
    "test_txt_file = \"/data/NNDL/data/train_test_split/verification/verification_pairs_easy.txt\"\n",
    "test_root_dir = \"/data/NNDL/data/image\"\n",
    "\n",
    "# Create the test dataset and DataLoader\n",
    "test_dataset = TestCarPairDataset(txt_file=test_txt_file, root_dir=test_root_dir, transform=test_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8831\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (img1, img2), labels in test_dataloader:\n",
    "        img1, img2, labels = img1.cuda(), img2.cuda(), labels.cuda()\n",
    "        output1, output2 = model(img1, img2)\n",
    "        \n",
    "        similarity_scores = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        predictions = (similarity_scores > 0.5).float()  # Using 0.5 as the threshold\n",
    "        \n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "test_accuracy = correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the test dataset text file\n",
    "test_txt_file = \"/data/NNDL/data/train_test_split/verification/verification_pairs_medium.txt\"\n",
    "test_root_dir = \"/data/NNDL/data/image\"\n",
    "\n",
    "# Create the test dataset and DataLoader\n",
    "test_dataset = TestCarPairDataset(txt_file=test_txt_file, root_dir=test_root_dir, transform=test_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8742\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (img1, img2), labels in test_dataloader:\n",
    "        img1, img2, labels = img1.cuda(), img2.cuda(), labels.cuda()\n",
    "        output1, output2 = model(img1, img2)\n",
    "        \n",
    "        similarity_scores = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        predictions = (similarity_scores > 0.5).float()  # Using 0.5 as the threshold\n",
    "        \n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "test_accuracy = correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the test dataset text file\n",
    "test_txt_file = \"/data/NNDL/data/train_test_split/verification/verification_pairs_hard.txt\"\n",
    "test_root_dir = \"/data/NNDL/data/image\"\n",
    "\n",
    "# Create the test dataset and DataLoader\n",
    "test_dataset = TestCarPairDataset(txt_file=test_txt_file, root_dir=test_root_dir, transform=test_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5618\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (img1, img2), labels in test_dataloader:\n",
    "        img1, img2, labels = img1.cuda(), img2.cuda(), labels.cuda()\n",
    "        output1, output2 = model(img1, img2)\n",
    "        \n",
    "        similarity_scores = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        predictions = (similarity_scores > 0.5).float()  # Using 0.5 as the threshold\n",
    "        \n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "test_accuracy = correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
